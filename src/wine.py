# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PeDafmXHwXlJV9c7QB9cne0bAIyw3sn6
"""

# INF1823 – Projet : Caractéristiques de vins italiens
# A) Corrélation / visualisation (+ FFT)
# B) Classification / valeurs manquantes
# C) Réduction de dimension / impact sur la prédiction
# [Bonus] D) Validation / surapprentissage vs sous-apprentissage


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.datasets import load_wine, load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import LogisticRegression

from numpy.fft import fft


# Chargement des données Wine

wine = load_wine()
X_wine = pd.DataFrame(wine.data, columns=wine.feature_names)
y_wine = wine.target

print("Nombre de lignes et de colonnes :", X_wine.shape)
print("Colonnes Wine   :", list(X_wine.columns))
print()


# A) Corrélation et visualisation (+ transformée de Fourier)


# 1) Matrice de corrélation
corr = X_wine.corr()

# On cherche la paire de variables la plus corrélée (|corr| > 0.5)
corr_no_diag = corr.where(~np.eye(corr.shape[0], dtype=bool))
max_pair = None
max_val = 0

for i in corr.index:
    for j in corr.columns:
        val = corr.loc[i, j]
        if pd.notna(val) and abs(val) > abs(max_val):
            max_val = val
            max_pair = (i, j)

var1, var2 = max_pair
print("Paire la plus corrélée : ", var1, "-", var2)
print("Coefficient de corrélation de Pearson :", max_val)
print()

# 2) Visualisation de la relation
plt.figure(figsize=(6, 4))
plt.scatter(X_wine[var1], X_wine[var2], alpha=0.7)
plt.xlabel(var1)
plt.ylabel(var2)
plt.title(f"Relation entre {var1} et {var2}\nCorrélation = {max_val:.3f}")
plt.tight_layout()
plt.show()

# 3) Exemple simple avec transformée de Fourier (sur var1)
signal = X_wine[var1].values
fft_vals = np.abs(fft(signal))
freqs = np.fft.fftfreq(len(signal))

# On affiche juste le module de la FFT (partie positive)
mask_pos = freqs >= 0
plt.figure(figsize=(6, 4))
plt.plot(freqs[mask_pos], fft_vals[mask_pos])
plt.xlabel("Fréquence")
plt.ylabel("Amplitude")
plt.title(f"Transformée de Fourier de la variable {var1}")
plt.tight_layout()
plt.show()


# B) Classification et gestion des données manquantes


# On normalise un peu pour la classification
X_train, X_test, y_train, y_test = train_test_split(
    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine
)

clf_knn = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=5))
])
clf_knn.fit(X_train, y_train)

print("Accuracy KNN (Wine, split 70/30) :", clf_knn.score(X_test, y_test))
print()

# 1) Prédiction de la classe pour deux nouvelles observations
test = [11, 1, 1, 1, 100, 1, 1, 1, 1, 1, 1, 1, 111]
new_d = [13, 2, 2, 20, 99, 2, 2, 0.4, 2, 5, 1, 2.5, 500]

pred_test = clf_knn.predict([test])[0]
pred_new_d = clf_knn.predict([new_d])[0]

print("Classe prédite pour 'test'  :", pred_test)
print("Classe prédite pour 'new_d' :", pred_new_d)
print()

# 2) Imputation des valeurs manquantes dans new_d
# new_d_miss = [X, X, 3, 15, 80, 3, 1, 0.3, 2, 5, 1, 2.5, 500]
new_d_miss = [np.nan, np.nan, 3, 15, 80, 3, 1, 0.3, 2, 5, 1, 2.5, 500]

imp_mean = SimpleImputer(strategy="mean")
imp_mean.fit(X_wine)
new_d_imputed = imp_mean.transform([new_d_miss])[0]

print("new_d avec valeurs manquantes imputées (moyenne) :")
for name, val in zip(X_wine.columns, new_d_imputed):
    print(f"  {name:30s} = {val:.4f}")
print()

pred_imputed = clf_knn.predict([new_d_imputed])[0]
print("Classe prédite pour new_d (avec imputation) :", pred_imputed)
print()


# C) Réduction de dimension et impact sur la prédiction


def evaluate_dim_reduction(X, y, name, transformer):
    """Entraîne un KNN sur X transformé et retourne l'accuracy moyenne."""
    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("dimred", transformer),
        ("knn", KNeighborsClassifier(n_neighbors=5))
    ])
    pipe.fit(X_tr, y_tr)
    acc = pipe.score(X_te, y_te)
    print(f"{name:20s} -> accuracy = {acc:.3f}")
    return acc

print(" Réduction de dimension sur Wine ")
# Baseline sans réduction
pipe_base_wine = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=5))
])
pipe_base_wine.fit(X_train, y_train)
acc_base_wine = pipe_base_wine.score(X_test, y_test)
print(f"{'Baseline (sans réduction)':20s} -> accuracy = {acc_base_wine:.3f}")

# PCA (quelques nombres de composantes)
for k in [2, 3, 5]:
    evaluate_dim_reduction(X_wine, y_wine, f"PCA (k={k})", PCA(n_components=k))

# LDA (n_components <= nb_classes-1)
evaluate_dim_reduction(X_wine, y_wine, "LDA (k=2)", LinearDiscriminantAnalysis(n_components=2))

# SelectKBest
for k in [3, 5, 8]:
    evaluate_dim_reduction(X_wine, y_wine, f"SelectKBest (k={k})", SelectKBest(f_classif, k=k))

print()

# --- Sur Iris ---
iris = load_iris()
X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)
y_iris = iris.target

X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(
    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris
)

print("Réduction de dimension sur Iris ")
# Baseline
pipe_base_iris = Pipeline([
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier(n_neighbors=5))
])
pipe_base_iris.fit(X_train_i, y_train_i)
acc_base_iris = pipe_base_iris.score(X_test_i, y_test_i)
print(f"{'Baseline (sans réduction)':20s} -> accuracy = {acc_base_iris:.3f}")

# PCA
for k in [2, 3]:
    evaluate_dim_reduction(X_iris, y_iris, f"PCA (k={k})", PCA(n_components=k))

# LDA (2 composantes max pour Iris)
evaluate_dim_reduction(X_iris, y_iris, "LDA (k=2)", LinearDiscriminantAnalysis(n_components=2))

# SelectKBest
for k in [2, 3, 4]:
    evaluate_dim_reduction(X_iris, y_iris, f"SelectKBest (k={k})", SelectKBest(f_classif, k=k))

print()


# BONUS D) Robustesse : surapprentissage / sous-apprentissage
#      + modèle polynomial d’ordre 1 + validation croisée


print("BONUS : Validation croisée (modèle polynomial d’ordre 1)")

# Ici : on crée un pipeline PolynomialFeatures(degree=1) + StandardScaler + LogisticRegression
# degree=1 => modèle linéaire, donc peu de risque de surapprentissage

poly_logit = Pipeline([
    ("poly", PolynomialFeatures(degree=1, include_bias=False)),
    ("scaler", StandardScaler()),
    ("logit", LogisticRegression(max_iter=500))  # multi_class auto par défaut
])

scores_cv = cross_val_score(poly_logit, X_wine, y_wine, cv=5)

print("Scores CV (Wine, poly degree=1, logistic) :", scores_cv)
print("Moyenne des scores CV :", scores_cv.mean())